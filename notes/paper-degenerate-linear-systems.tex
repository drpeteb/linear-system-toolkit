\documentclass[journal,10pt]{IEEEtran}
% \documentclass[11pt,draftcls,onecolumn]{IEEEtran}
% \documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{cite}

\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
 \usetikzlibrary{plotmarks}
 \pgfplotsset{compat=newest}
 \pgfplotsset{plot coordinates/math parser=false}
 \usepgfplotslibrary{external}
 \tikzexternalize[prefix=tikz/]

\input{linear-system-learning-macros.tex}
\newcommand{\meta}[1]{{\color{red}\em #1}}

%opening
\title{Bayesian Learning of Degenerate Linear~Gaussian~State~Space~Models using~Markov~Chain~Monte~Carlo}
\author{Pete Bunch$^*$, James Murphy and Simon Godsill
  \thanks{Department of Engineering, University of Cambridge, Cambridge, UK.}
  \thanks{Email: $\{$pb404,jm362,sjg30$\}$@cam.ac.uk}%
}

\begin{document}

\maketitle



\begin{abstract}
Linear Gaussian state space models are ubiquitous in signal processing, and an important procedure is that of estimating system parameters from observed data. Rather than making a single point estimate, it is often desirable to conduct Bayesian learning, in which the entire posterior distribution of the unknown parameters is sought. This can be achieved using Markov chain Monte Carlo. On some occasions it is possible deduce the form of the unknown system matrices in terms of a small number of scalar parameters, by considering the underlying physical processes involved. Here we study the case where this is not possible, and the entire matrices must be treated as unknowns. An efficient Gibbs sampling algorithm exists for the basic formulation of linear model. We extend this to the more challenging situation where the transition model is degenerate. Appropriate Markov kernels are devised and demonstrated with simulations.
\end{abstract}



\section{Introduction}

State space models are frequently used to describe time-varying systems, and Linear Gaussian models in particular are ubiquitous throughout signal processing. The physical phenomena modelled within this framework include the kinematics of moving targets \cite{Bar-Shalom2002}, audio signals \cite{Godsill1998}, genetic networks \cite{Beal2005}, and many others. The popularity of linear Gaussian state space models stems in part from their analytic tractability. Inference tasks can be performed in closed form using algorithms such as the Kalman filter \cite{Kalman1960} and Rauch-Tung-Striebel (RTS) smoother \cite{Rauch1965}.

A linear Gaussian state space model is specified by a number of system matrices which govern the latent state and observation processes. An important consideration is how to learn these fixed matrices from observed data. The parametric approach to this problem is to deduce the form of the system matrices in terms of a small number of scalar parameters by considering the physical mechanisms underlying the system, and then attempt to learn these. (See e.g. \cite{Kantas2009,Andrieu2010}.) However, in many systems there may not be an obvious parametric form, and it may be better to treat each system matrix in its entirety as an unknown variable to be learned. This type of non-parametric learning for linear Gaussian state space models is the focus of this paper. There is a large body of research on this topic, which has generally concentrated on point estimate methods. These include subspace identification approaches \cite{VanOverschee1991,Viberg1995}, and maximum likelihood estimates using either gradient-based algorithms (see e.g. \cite{Cappe2005,Sarkka2013}) or Expectation-Maximisation (EM) \cite{Shumway1982,Digalakis1993,Ghahramani1996,Gibson2005,Li2009}.

More recently, Bayesian approaches have been developed for non-parametric learning of linear Gaussian state space models, in which full posterior distributions for the system parameters are calculated. Since this is not possible analytically (see discussion in \cite{Beal2003}), approximations must be employed. One possibility is to take a variational approach, approximating the full joint posterior distribution as a product of independent marginals \cite{Ghahramani2001,Beal2003,Barber2007}. Alternatively, the true joint posterior can be targeted using Markov chain Monte Carlo (MCMC). This has the advantage of providing consistent estimates not only of the system matrices but also of any function of these quantities (e.g. phase margins, system poles). The price we pay is in computation; it is necessary to allow the Markov chains to run until convergence in order to form good estimates. Generic Metropolis-Hastings strategies such as those adopted in \cite{Ninness2010} are liable to mix very slowly and exact too great a computational demand. However, if conjugate priors are assumed for the various system matrices, then a Gibbs sampler may be implemented which attains substantially improved efficiency and requires almost no algorithm tuning \cite{Wills2012}.

The Gibbs sampler approach developed in \cite{Wills2012} is effective on the basic flavour of linear Gaussian state space models. However, it is not able to handle the case in which the transition model is degenerate, i.e. when the transition covariance matrix is not full rank. In this paper we extend the MCMC learning framework to encompass this scenario. This is achieved by appropriate factorisations of the covariance matrix which allow us still to exploit the fast-mixing Gibbs kernel. Furthermore, reversible jump construction \cite{Green1995,Green2009} is introduced which enables the sampler to learn the rank of the transition covariance matrix.

%and parameterising the attainable subspace using a set of Givens rotations \cite{Anderson1987,Shalit2014,Yang1994,Cao2011,Cron2014}.

The paper is structured as follows. In section~\ref{sec:linear_gaussian_models} we review the Gibbs sampler for basic linear Gaussian state space systems, focusing on the transition model. In section~\ref{sec:degenerate_transition_models} we describe modifications for degenerate transitions. Simulations on a toy model, and a motion capture application are presented in section~\ref{sec:applications}.


% Citations which might be useful
% Linear systems with stability constraints: \cite{Boots2008}
% Givens rotations: \cite{Yang1994,Cao2011,Cron2014} (for covariance) \cite{Anderson1987,Shalit2014} (for generic orthogonal matrixes)



\section{Basic Linear Gaussian Models} \label{sec:linear_gaussian_models}
State space models are used to represent time-varying systems, and consist of a latent state process $\{\ls{\ti}\}_{\ti=1:\timax}$ and a related observation process $\{\ob{\ti}\}_{\ti=1:\timax}$. The challenge is to infer the sequence of unknown latent states, and learn parameters of the model, from the sequence of known observations. It is usually assumed that the state process is Markovian (i.e. $\ls{\ti}|\ls{\ti-1}$ is independent of $\ls{1:\ti-2}$) and that each observation depends only on the current state (i.e. $\ob{\ti}|\ls{\ti}$ is independent of $\ls{1:\ti-1}$ and $\ls{\ti+1:\timax}$).

A basic linear state space model therefore obeys the following recursive system equations,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \lgtm \ls{\ti-1} + \tn{\ti} \\
 \ob{\ti} & = & \lgom \ls{\ti}   + \on{\ti}       ,
\end{IEEEeqnarray}
%
where $\ls{\ti} \in \reals^{\lsd}$ and $\ob{\ti} \in \reals^{\obd}$. In addition, we assume the state disturbance and observation noise variables have a Gaussian distribution,
%
\begin{IEEEeqnarray}{rCl}
 \tn{\ti} & \sim & \normalden{\tn{\ti}}{0}{\lgtv} \\
 \on{\ti} & \sim & \normalden{\on{\ti}}{0}{\lgov}     ,
\end{IEEEeqnarray}
%
where $\lgtv$ and $\lgov$ are positive definite covariance matrices. This model can be written equivalently in terms of transition and observation densities,
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) & = & \normalden{\ls{\ti}}{\lgtm\ls{\ti-1}}{\lgtv} \\
 \den(\ob{\ti}|\ls{\ti},\lgom,\lgov)   & = & \normalden{\ob{\ti}}{\lgom\ls{\ti}}{\lgov}      .
\end{IEEEeqnarray}
%
The initial state $\ls{1}$ may be known or may be assigned a Gaussian prior.

The system is parameterised by the four matrices $\left\{\lgtm, \lgom, \lgtv, \lgov\right\}$. In this paper we focus on learning the transition model, i.e. $\lgtm$ and $\lgtv$, and treat $\lgom$ and $\lgov$ as fixed and known throughout. Conditional on these matrices, state inference may be carried out analytically. Posterior filtering and smoothing densities may be calculated using the Kalman filter \cite{Kalman1960} and Rauch-Tung-Striebel (RTS) smoother \cite{Rauch1965}. Furthermore, the Kalman filter may also be used to evaluate the marginal likelihood $\den(\ob{1:\timax}|\lgtm, \lgtv)$, and it is possible to draw posterior samples from the state posterior $\den(\ls{1:\timax}|\ob{1:\timax}, \lgtm, \lgtv)$ using the forward-filtering-backward-sampling method \cite{Chib1996}. The purpose of a learning algorithm is estimate unknown values of $\left\{\lgtm, \lgtv\right\}$ from a sequence of observations. Within the Bayesian framework this means calculating or approximating the posterior distribution $\den(\lgtm, \lgtv|\ob{1:\timax})$, which we can achieve using MCMC. 

In order to conduct Bayesian learning, we first need to define prior distributions for the unknown system matrices $\lgtm$ and $\lgtv$. Although these should be selected to reflect prior belief about the system in question, there is substantial benefit in using conjugate priors, as these enable us to use Gibbs sampling moves. Typically, we will not have strong prior beliefs about the parameters, so it is reasonable to use an uninformative conjugate prior. However, if we do have prior knowledge to take into account which means that a conjugate prior is not appropriate, then we can treat each Gibbs move as a proposal in a Metropolis-Hastings scheme and use an accept/reject stage to account for the difference between the true prior and the conjugate prior used for the sampling \cite{Wills2012}.



\subsection{System Matrix Priors}

The conjugate prior for $\lgtm, \lgtv$ is a matrix normal--inverse Wishart distribution \cite{Wills2012},
%
\begin{align}
 \lgtv &\sim \iwishartdist{\priordof}{\priorscalematrix} \\
 \lgtm | \lgtv &\sim \matrixnormaldist{\priormeanmatrix}{\lgtv}{\priorcolumnvariance}     ,
\end{align}
%
with the following density function,
%
\begin{equation}
 \den(\lgtm, \lgtv) =  \den(\lgtm | \lgtv) \den(\lgtv)
\end{equation}
%
\begin{align}
\den(\lgtv) &= \frac{ \determ{\priorscalematrix}^{\frac{\priordof}{2}} }{ 2^{\frac{\priordof}{2}} \gammafun\left(\frac{\priordof}{2}\right) } \determ{\lgtv}^{-\frac{\priordof+\lsd+1}{2}} \exp\left( -\half \trace\left[\lgtv\inv\priorscalematrix\right] \right) \\
\den(\lgtm | \lgtv) &= \determ{2 \pi \lgtv}^{-\half} \determ{2 \pi \priorcolumnvariance}^{-\half} \exp\left(-\half \trace\left[ \lgtv\inv (\lgtm-\priormeanmatrix) \priorcolumnvariance\inv (\lgtm-\priormeanmatrix)\tr \right] \right) 
\end{align}
%
$\priordof\in\reals, \priordof>\lsd-1$. $\priorscalematrix$ and $\priorcolumnvariance$ are $\lsd\times\lsd$ positive definite matrices. $\priormeanmatrix \in \reals^{\lsd\times\lsd}$.



\subsection{Gibbs Sampling for Basic Linear Gaussian Models}

Our principal interest is to learn the system matrices $\lgtm$ and $\lgtv$ from a sequence of observations $\ob{1:\timax}$. The appropriate posterior distribution is not amenable to efficient MCMC sampling, so instead we introduce the latent state sequence $\ls{1:\timax}$ as a nuisance variable and target the joint posterior,
%
\begin{IEEEeqnarray}{rCl}
 \postden(\lgtm, \lgtv, \ls{1:\timax}) & = & \den(\lgtm, \lgtv, \ls{1:\timax} | \ob{1:\timax}) \\
 & \propto & \den(\ob{1:\timax}|\ls{1:\timax}) \den(\ls{1:\timax}|\lgtm,\lgtv) \den(\lgtm|\lgtv) \den(\lgtv) \nonumber      .
\end{IEEEeqnarray}

An appropriate Gibbs sampler can be constructed by sampling alternately from the following conditional posterior distributions,
%
\begin{IEEEeqnarray}{l}
 \postden(\ls{1:\timax}|\lgtm, \lgtv) \nonumber \\
 \postden(\lgtm, \lgtv| \ls{1:\timax}) \nonumber      .
\end{IEEEeqnarray}
%
The distribution of the sampled values will then converge to the target posterior distribution \cite{Roberts1994}. The state conditional $\postden(\ls{1:\timax}|\lgtm, \lgtv)$ can be sampled using the forward-filtering-backward-sampling algorithm \cite{Chib1996,Wills2012}, which works by first running a Kalman filter forwards through the data, then running backwards in time sampling each state conditional on those in the future. The parameter conditional may also be sampled directly. This depends on the state sequence probability,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \den(\ls{1:\timax}|\lgtm,\lgtv) \propto \prod_{\ti=2}^{\timax} \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) } \\
 \quad\quad & \propto & \determ{\lgtv}^{-\frac{\timax-1}{2}} \exp\left( -\half \sum_{\ti=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})\tr \lgtv\inv (\ls{\ti}-\lgtm\ls{\ti-1}) \right)      .
\end{IEEEeqnarray} 

It is straightforward to show that the required conditional distribution is also a matrix normal--inverse Wishart distribution \cite{Wills2012},
%
\begin{IEEEeqnarray}{rCl}
 \lgtv | \ls{1:\timax} &\sim& \iwishartdist{\postdof}{\postscalematrix} \\
 \lgtm | \lgtv, \ls{1:\timax} &\sim& \matrixnormaldist{\postmeanmatrix}{\lgtv}{\postcolumnvariance}     ,
\end{IEEEeqnarray}
%
with the following updated hyperparameters,
%
\begin{align}
 \postcolumnvariance\inv                 &= \priorcolumnvariance\inv + \suffstats{1} \\
 \postmeanmatrix \postcolumnvariance\inv &= \priormeanmatrix \priorcolumnvariance\inv + \suffstats{2}\\
 \postdof                                &= \priordof + \suffstats{0} \\
 \postscalematrix                        &= \priorscalematrix + \suffstats{3} + \priormeanmatrix \priorcolumnvariance\inv \priormeanmatrix\tr - \postmeanmatrix \postcolumnvariance\inv \postmeanmatrix\tr    ,
\end{align}
%
in which the following sufficient statistics are used,
%
\begin{align}
 \suffstats{0} &= \timax - 1 \\
 \suffstats{1} &= \sum_{t=2}^{\timax} \ls{\ti-1}\ls{\ti-1}\tr \\
 \suffstats{2} &= \sum_{t=2}^{\timax} \ls{\ti}\ls{\ti-1}\tr \\
 \suffstats{3} &= \sum_{t=2}^{\timax} \ls{\ti}\ls{\ti}\tr      .
\end{align}

Both matrix normal and inverse Wishart distributions may be sampled using standard methods. Hence we have all the components necessary to implement a Gibbs sampler for this model.



\section{Degenerate Transition Models} \label{sec:degenerate_transition_models}

A commonly encountered variation on the basic linear Gaussian state space model is as follows,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \lgtm \ls{\ti-1} + \lgdm \tn{\ti} \label{eq:degenerate_transition} \\
 \tn{\ti} & \sim & \normalden{\tn{\ti}}{0}{\idmat}      ,
\end{IEEEeqnarray}
%
in which $\tn{\ti}$ as dimension $\rk$, and $\lgdm$ is $\lsd\times\rk$. This is almost equivalent to the previous, with $\lgtv=\lgdm\lgdm\tr$. However, whereas before $\lgtv$ was required to be positive definite, this is no longer the case. When this occurs, the conventional transition density $\den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgdm)$ is not defined, and the sampling operations underlying our Gibbs sampler are no longer valid. Degenerate models such as these arise in tracking applications (see \cite{Maskell2004} for a discussion of this phenomenon); when non-Markovian models, such as autoregressive process, are converted to state space form; and when it is natural to parameterise a system with more latent states than there are degrees of freedom (see section~\ref{sec:mocap}). In this section, we introduce a parameterisation for such degenerate transition models which can uniquely represent any such system, and show how MCMC moves may be implemented for learning.

The matrix $\lgdm$ is not uniquely identifiable from the state sequence, because the distribution of $\lgdm\tn{\ti}$ is identical to that of $\lgdm \Xi \tn{\ti}$ where $\Xi$ is an arbitrary orthogonal matrix. To avoid this ambiguity, it is better to focus on learning $\lgtv$ as before, and to define $\lgdm$ as some matrix square root of $\lgtv$. In this new setting, $\lgtv$ is a positive semi-definite matrix with rank $\rk$.

There are two difficulties with extending the methodology from the basic model to the degenerate case. First, the conjugate matrix normal--inverse Wishart distribution becomes singular. This places an undesirable constraint on the transition matrix which needs to be removed. Second, each transition, $(\ls{\ti}-\lgtm\ls{\ti-1})$, is now constrained to lie in a particular $\rk$-dimensional subspace within the state space; specifically, within the column space of $\lgdm$. This subspace is fixed conditional on any sampled state trajectory $\ls{1:\timax}$ (provided that $\timax>\rk$). This makes standard Gibbs sampling from $\postden(\lgtm, \lgtv|\ls{1:\timax})$ insufficient, since there are values of $\lgtm$ and $\lgtv$ which can never be reached.

There is an obvious way to avoid being constrained to a particular subspace by the sampled state trajectory; simply return to targeting the marginal posterior $\postden(\lgtm, \lgtv|\ob{1:\timax})$ and fall back to using a generic Metropolis-Hastings approach. However, with a complex and high dimensional parameter space, designing efficient proposal distributions for Metropolis-Hastings is difficult. We would prefer to attain the automated, fast-mixing properties displayed by the Gibbs sampler on the full rank model. In this section we show how a Gibbs kernel may still be applied to sample a component of the parameters, with the remainder handled using Metropolis-Hastings.

In formulating an MCMC algorithm for degenerate transition models, we will make use of two factorisations of the transition covariance matrix. First, an eigendecomposition,
%
\begin{IEEEeqnarray}{rCl}
 \lgtv &=& \tvvec \tvval \tvvec\tr     ,
\end{IEEEeqnarray}
%
in which $\tvval$ is a diagonal $\rk\times\rk$ matrix with $\rk = \rank(\lgtv)$, and $\tvvec$ is a $\lsd\times\rk$ matrix of orthonormal columns from the appropriate Stiefel manifold. This is the ``non-singular'' part of the factorisation. In order to resolve ambiguities in this representation, we follow \cite{Muirhead1982} and require that the first element in each column of $\tvval$ should be positive, and that eigenvalues in $\tvval$ be arranged in decreasing order of magnitude. The transformation from $\lgtv$ to $\{\tvval, \tvvec\}$ is then bijective.

We can also write the ``full'' factorisation by finding a set of $\lsd-\rk$ additional orthonormal vectors and appending them to $\tvvec$, along with matching zeros to $\tvval$, as follows,
%
\begin{IEEEeqnarray}{rCl}
 \lgtv &=& \begin{bmatrix}\tvvec & \tvvecorth \end{bmatrix} \begin{bmatrix}\tvval & \zmat \\ \zmat & \zmat \end{bmatrix} \begin{bmatrix}\tvvec\tr \\ \tvvecorth\tr \end{bmatrix}
\end{IEEEeqnarray}
%
Note that $\tvvecorth$ is not unique. 

A further factorisation will also be useful, which we will refer to as the Givens decomposition (on account of its method of computation). For the rank-$\rk$ positive semi-definite covariance matrix $\lgtp$,
%
\begin{IEEEeqnarray}{rClCl}
 \lgtv &=& \tvrot \tvfull \tvrot\tr &=& \begin{bmatrix}\tvrot & \tvrotorth\end{bmatrix} \begin{bmatrix}\tvfull & \zmat \\ \zmat & \zmat\end{bmatrix} \begin{bmatrix}\tvrot\tr \\ \tvrotorth\tr\end{bmatrix} \label{eq:givens_decomposition}
\end{IEEEeqnarray}
%
where,
%
$\tvfull$ is a $\rk\times\rk$ positive definite matrix and $\tvrot$ is a $\lsd\times\rk$ matrix of orthonormal vectors subject to certain constraints. The transformation from $\lgtv$ to $\{\tvfull, \tvrot\}$ is also bijective. See appendix~\ref{app:givens-factorisation} for details.


\subsection{Degenerate Transition Priors}

\meta{Handle these consistently.}

The matrix normal--inverse Wishart conjugate prior will need some modifications in order to apply it to degenerate models.

An appropriate prior distribution for the degenerate transition covariance matrix may be defined by setting the degrees of freedom of an inverse Wishart distribution to the desired rank, $\priordof=\rk$. This results in a singular inverse Wishart distribution over the rank-$\rk$ positive semi-definite matrices \cite{Diaz-Garcia2006}. The density (with respect to the Lesbegue measure on the distinct elements),
%
\begin{IEEEeqnarray}{rCl}
 \den(\lgtv|\rk) &=& \frac{ \determ{\priorscalematrix}^{\frac{\rk}{2}} }{ 2^{\half\rk\lsd} \pi^{\half\rk(\lsd-\rk)} \gammafun[r]\left(\frac{\rk}{2}\right) } \determ{\tvval}^{-\half(3\lsd-\rk+1)} \exp\left( -\half \lgtv\pinv \priorscalematrix \right)    ,
\end{IEEEeqnarray}
%
in which $\lgtv\pinv$ denotes the Moore-Penrose pseudoinverse of $\lgtv$.

The matrix normal component of the prior is more problematic. Although the distribution it is still well-defined, the fact that $\lgtv$ is singular means that the space of possible values is restricted to a subspace of $\reals^{\lsd\times\lsd}$, placing an unwanted constraint on our model. (See \cite{Diaz-Garcia2006} for a discussion of the singular matrix normal distribution.)

In order to relax this unwanted constraint, the following non-singular prior distribution may be used,
%
\begin{IEEEeqnarray}{rCl}
 \lgtm|\lgtv &\sim& \matrixnormaldist{\priormeanmatrix}{\lgtv+\tvvecorth\priorextratpval\tvvecorth\tr}{\priorcolumnvariance}
\end{IEEEeqnarray}
%
in which $\priorextratpval$ is a diagonal matrix of positive eigenvalues. The matrix $\lgtv+\tvvecorth\priorextratpval\tvvecorth\tr$ is then guaranteed to be positive definite.

Using this prior, we need a mechanism for choosing $\priorextratpval$. These extra eigenvalues control the rate at which probability decays as the matrix moves away from the subspace defined by $\lgtv$.
\meta{Add something about how to set these.}

Finally, the prior is completed by specifying a distribution over the possible values of the rank $\den(\rk)$.


\subsection{Constrained Gibbs Sampling for Degenerate Transitions}

As discussed, standard Gibbs sampling using $\postden(\lgtm, \lgtv|\ls{1:\timax})$ does not allow the Markov chain to fully explore the parameter space because of the constraint that each transition $(\ls{\ti}-\lgtm\ls{\ti-1})$ must lie in the $\rk$-dimensional column space of $\lgdm$. Nevertheless, we can sample from these distributions exactly, and doing so allows the chain to efficiently explore values of the transition matrix and transition covariance within the permitted regions. These constrained Gibbs sampling steps rely on the Givens decomposition \eqref{eq:givens_decomposition}.

\subsubsection{Likelihood and Subspace Constraints}
There is no transition density over $\reals^{\lsd}$ for the degenerate model. However, a density does exist with respect to an appropriate measure on the reachable subspace, which provides us with a likelihood function \cite{Diaz-Garcia2006}. We can derive this from the underlying state transition equation,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} &=& \lgtm \ls{\ti-1} + \tvrot \tvfull\msqrt \tn{\ti} \nonumber
 \end{IEEEeqnarray}
 \begin{IEEEeqnarray}{rCl}
 \Rightarrow \tvrot\tr(\ls{\ti}-\lgtm\ls{\ti-1}) &\sim& \normaldist{\zmat}{\tvfull} \nonumber
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) &\propto& \determ{\tvfull}^{-\half} \exp\left(-\half (\ls{\ti}-\lgtm\ls{\ti-1})\tr \tvrot \tvfull\inv \tvrot\tr (\ls{\ti}-\lgtm\ls{\ti-1}) \right)     .
\end{IEEEeqnarray}
%
The subspace over which this is defined is,
%
\begin{IEEEeqnarray}{rCl}
 \{\ls{\ti} : \ls{\ti} = \lgtm \ls{\ti-1} + \tvrot z, z \in \reals^{\rk}\}     .
\end{IEEEeqnarray}

The likelihood function for the entire state trajectory is thus,
%
\begin{align}
 \den(\ls{1:\timax}|\lgtm, \lgtv) &= \den(\ls{0}) \prod_{t=2}^{\timax} \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) \nonumber \\
 &\propto \determ{\tvfull}^{-\half(\timax-1)} \exp\left( -\half \sum_{t=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})\tr \tvrot \tvfull\inv \tvrot\tr (\ls{\ti}-\lgtm\ls{\ti-1}) \right) \nonumber \\
 &= \determ{\tvfull}^{-\half(\timax-1)} \exp\left(-\half\trace\left[ \tvfull\inv \tvrot\tr \sum_{t=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})(\ls{\ti}-\lgtm\ls{\ti-1})\tr \tvrot \right]\right) \nonumber \\
 &= \determ{\tvfull}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \tvfull\inv \left( \tvrot\tr \lgtm \suffstats{1} \lgtm\tr \tvrot - \tvrot\tr \lgtm \suffstats{2}\tr \tvrot - \tvrot\tr \suffstats{2} \lgtm\tr \tvrot + \tvrot\tr \suffstats{3} \tvrot \right) \right]\right) \\
 &= \determ{\tvfull}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \tvfull\inv \left( \lgtmrot \suffstats{1} \lgtmrot\tr - \lgtmrot \suffstats{2}\tr \tvrot - \tvrot\tr \suffstats{2} \lgtmrot\tr + \tvrot\tr \suffstats{3} \tvrot \right) \right]\right)
\end{align}
%
in which the sufficient statistics are as before and,
%
\begin{equation}
 \lgtmrot = \tvrot\tr \lgtm      .
\end{equation}

Writing all the subspace constraints concisely in a single equation,
%
\begin{IEEEeqnarray}{rCl}
 \LS{2:\timax} &=& \lgtm \LS{1:\timax-1} + \tvrot Z
\end{IEEEeqnarray}
%
where $Z \in \reals^{\rk\times\lsd}$ and 
%
\begin{IEEEeqnarray}{rCl}
 \LS{2:\timax} &=& \begin{bmatrix} \ls{2} & \ls{3} & \hdots & \ls{\timax} \end{bmatrix}    .
\end{IEEEeqnarray}

Now introduce,
%
\begin{IEEEeqnarray}{rCl}
 \lgtmorth &=& (\idmat-\tvrot\tvrot\tr) \lgtm     ,
\end{IEEEeqnarray}
%
such that the transition matrix may be decomposed into orthogonal components,
%
\begin{IEEEeqnarray}{rCl}
 \lgtm &=& \tvrot \lgtmrot + \lgtmorth    .
\end{IEEEeqnarray}
%
Using this, the constraint equation becomes,
%
\begin{IEEEeqnarray}{rCl}
 \LS{2:\timax} &=& (\tvrot \lgtmrot + \lgtmorth) \LS{1:\timax-1} + \tvrot Z \nonumber \\
 \Rightarrow \LS{2:\timax} &=& \lgtmorth \LS{1:\timax-1} + \tvrot Z' \nonumber      .
\end{IEEEeqnarray}
%
Notice that $\lgtmrot$ and $\tvfull$ do not appear in this equation, and thus may be freely altered by the sampler. In contrast, $\lgtmorth$ and $\tvrot$ are uniquely determined. Sampling from $\postden(\lgtm,\lgtv|\ls{1:\timax})$ thus entails drawing new values from $\postden(\lgtmrot,\tvfull|\lgtmorth,\tvrot,\ls{1:\timax})$.

\subsubsection{Transforming the Prior}
We will need the distribution on $\{\lgtmrot, \tvfull\}$ implied by the prior we chose for $\{\lgtm,\lgtv\}$. These follow from simple properties of the wishart and matrix normal distributions (see \cite{Muirhead1982}). For the matrix normal part,
%
\begin{IEEEeqnarray}{rCl}
 \lgtmrot|\tvrot,\tvfull &\sim& \matrixnormaldist{\tvrot\tr\priormeanmatrix}{\tvfull}{\priorcolumnvariance}     .
\end{IEEEeqnarray}
%
For the inverse Wishart part,
%
\begin{IEEEeqnarray}{rCl}
 \tvfull|\tvrot &\sim& \iwishartdist{\rk}{(\tvrot\tr\priorscalematrix\inv\tvrot)\inv}     .
\end{IEEEeqnarray}
%
Notice that this is no longer a singular Inverse Wishart distribution, since $\tvfull$ is $\rk\times\rk$.


\subsubsection{Sampling the Conditional}
Using the constrained likelihood function and the transformed prior, we now obtain the required posterior conditional distribution $\postden(\lgtmrot,\tvfull|\lgtmorth,\tvrot,\ls{1:\timax})$. This follows the same algebraic steps as in the full rank case and leads to a similar matrix normal--inverse Wishart distribution,
%
\begin{align}
 \tvfull | \tvrot, \ls{1:\timax} &\sim \iwishartdist{\postdof}{\postscalematrix} \\
 \lgtmrot | \tvfull, \tvrot, \ls{1:\timax} &\sim \matrixnormaldist{\postmeanmatrix}{\lgtv}{\postcolumnvariance}     ,
\end{align}
%
with the following updated hyperparameters,
%
\begin{align}
 \postcolumnvariance\inv                 &= \priorcolumnvariance\inv + \suffstats{1} \\
 \postmeanmatrix \postcolumnvariance\inv &= \tvrot\tr\left( \priormeanmatrix \priorcolumnvariance\inv + \suffstats{2} \right) \\
 \postdof                                &= \rk + \suffstats{0} \\
 \postscalematrix                        &= (\tvrot\tr\priorscalematrix\inv\tvrot)\inv + \tvrot\tr\left( \suffstats{3} + \priormeanmatrix \priorcolumnvariance\inv \priormeanmatrix\tr \right)\tvrot - \postmeanmatrix \postcolumnvariance\inv \postmeanmatrix\tr
\end{align}

Once new values of $\lgtmrot$ and $\tvfull$ have been sampled, the corresponding values of $\lgtm$ and $\lgtv$ may be calculated using the existing $\lgtmorth$ and $\tvrot$.


\subsection{Metropolis-Hastings on the Posterior Marginal}

Gibbs sampling is very efficient and requires no algorithm tuning but it only allows the Markov chain to explore a subset of the parameter space. In this section we introduce two Metropolis-Hastings kernels which target the marginal posterior $\postden(\lgtm,\lgtv)$ and allow the sampler to fully explore the target distribution. These sampling operations are each followed by a draw of the state trajectory from $\postden(\ls{1:\timax}|\lgtm,\lgtv)$, thus constituting a collapsed Gibbs move \cite{Dyk2008}.

\subsubsection{Covariance Random Walk}

The covariance matrix is specified by its eigenvalues and eigenvectors. The sampler is already able to explore the possible eigenvalues using the constrained Gibbs moves, but the eigenvector matrix is partially fixed during this process. We can allow the sampler to reach any possible eigenvector matrix using a rotational random walk. Suppose the current value of the transition covariance is $\lgtv\mcold$, a new value is proposed using the following procedure:
%
\begin{itemize}
 \item Sample an orthogonal matrix $\mhrot$ from some distribution $\mhrotppsl$.
 \item Set $\lgtv\mcnew = \mhrot \lgtv\mcold \mhrot\tr$.
\end{itemize}
%
The transformation is invertible and has a Jacobian of $1$. If in addition we require that $\mhrotppsl(\mhrot) = \mhrotppsl(\mhrot\tr)$, then the proposal distribution is symmetric and the acceptance probability is simply,
%
\begin{IEEEeqnarray}{rCl}
 \mhap(\lgtv\mcold\to\lgtv\mcnew) & = & \min\left\{1, \frac{ \postden(\lgtm, \lgtv\mcnew) }{ \postden(\lgtm,\lgtv\mcold) } \right\} \\
  & = & \min\left\{1, \frac{\den(\ob{1:\timax}|\lgtm,\lgtv\mcnew)}{\den(\ob{1:\timax}|\lgtm,\lgtv\mcold)} \times \frac{\den(\lgtm,\lgtv\mcnew)}{\den(\lgtm,\lgtv\mcold)} \right\} \nonumber     .
\end{IEEEeqnarray}


There are numerous ways to sample the rotation matrix $\mhrot$ from a suitable proposal distribution $\mhrotppsl$. For example, we could use the Cayley transform \cite{Leon2006}, a bijective mapping from the skew-symmetric matrices to the rotation matrices, defined by,
%
\begin{IEEEeqnarray}{rCl}
 P(S) & = & (\idmat - S)\inv(I+S)     .
\end{IEEEeqnarray}
%
To sample from $\mhrotppsl$, we draw $\half\lsd(\lsd-1)$ independent scalar random variables $\{s_{i,j}\}_{0<i<j<\lsd}$ from any zero-mean distribution; a nice choice is,
%
\begin{IEEEeqnarray}{rCl}
 s_{k,l} & \sim & \normaldist{0}{\sigma_Q^2} \label{eq:skewsymmetric_proposal}     .
\end{IEEEeqnarray}
%
Use these to construct a skew-symmetric matrix $S$,
%
\begin{IEEEeqnarray}{rCl}
 S_{k,l} & = & \begin{cases}
                s_{k,l}  & k<l \\
                -s_{l,k} & k>l \\
                0        & k=l     ,
               \end{cases}
\end{IEEEeqnarray}
%
and then set $\mhrot=P(S)$. The Cayley transform has the property that $P(-S)=P(S)\inv=P(S)\tr$, which implies that $\mhrotppsl(\mhrot)=\mhrotppsl(\mhrot\tr)$ as required.

An alternative is to use Givens rotations. First sample $i \in [1,\lsd]$, $j \in [1,\lsd]\setminus i$, and $\givrot{} \in [-\pi/2,\pi/2]$ from some zero-mean distribution. Form the Givens matrix $\givmat{i}{j}{\givrot{}}$ such that,
%
\begin{IEEEeqnarray}{rCl}
 \left[\givmat{i}{j}{\givrot{}} - \idmat\right]_{k,l} & = & \begin{cases}
                                                    \cos(\givrot{})-1 & k=l=i \text{ or } k=l=j \\
                                                    \sin(\givrot{}) & k=i,l=j \\
                                                    -\sin(\givrot{}) & k=j,l=i \\
                                                    0 & \text{ otherwise,}
                                                 \end{cases}
\end{IEEEeqnarray}
%
and use $\mhrot=\givmat{i}{j}{\givrot{}}$. This also has the property that $\givmat{i}{j}{-\givrot{}} = \givmat{i}{j}{\givrot{}}\tr$, implying a symmetric proposal.



\subsubsection{Transition Matrix Random Walk}

A more conventional Metropolis-Hastings kernel can be used for the transition matrix; for example, using a simple Gaussian random walk as the proposal distribution. If the current value is $\lgtm\mcold$, a new matrix is sampled from a matrix normal distribution,
%
\begin{IEEEeqnarray}{rCl}
 \lgtm\mcnew &\sim& \matrixnormaldist{\lgtm\mcold}{\sigma_F^2\idmat}{\idmat}     ,
\end{IEEEeqnarray}
%
and accepted with probability,
%
\begin{IEEEeqnarray}{rCl}
 \mhap(\lgtm\mcold\to\lgtm\mcnew) & = & \min\left\{1, \frac{ \postden(\lgtm\mcnew, \lgtv) }{ \postden(\lgtm\mcold,\lgtv) } \right\} \\
  & = & \min\left\{1, \frac{\den(\ob{1:\timax}|\lgtm\mcnew,\lgtv)}{\den(\ob{1:\timax}|\lgtm\mcold,\lgtv)} \times \frac{\den(\lgtm\mcnew,\lgtv)}{\den(\lgtm\mcold,\lgtv)} \right\} \nonumber     .
\end{IEEEeqnarray}



\subsection{Reversible Jump for Rank Learning}

Since it is unlikely that the rank of $\lgtv$ will be known a priori, a mechanism is needed which will allow the sampler to learn this parameters as well. Changing the rank of $\lgtv$ changes the number of independent elements, and therefore the dimension of the target probability distribution. Thus, we will need to use reversible jump MCMC for this task \cite{Green1995,Green2009}.

A reversible jump sampler is an MCMC algorithm on a family of probability distributions on spaces of different dimensions. Sampling operations are designed in pairs which allow the sampler to move between the different distributions. Each move begins by sampling additional variables (if required) and then applying a reversible transformation. Finally the newly proposed state is accepted or rejected according to a Metropolis-Hastings acceptance probability.

For this problem, there is a distribution for each possible value of the rank. Two moves are required, to increase and decrease the rank. The moves suggested here are by no means unique, but are intuitive, simple to implement, and have proven effective in simulations.

The reversible transformation can be accomplished in terms of eigenvectors and eigenvalues. Suppose the current value of the transition covariance is $\lgtv = \tvvec \tvval \tvvec\tr$, which has rank $\rk$. To increase the rank, first we sample an additional eigenvalue $\eval{\rk+1} \in [0,\eval{\rk}]$ (recall that the elements of $\tvval$ are sorted in decreasing order of magnitude, so $\eval{\rk+1} < \eval{\rk} < \dots < \eval{1}$) and a corresponding eigenvector $\evec{\rk+1} \in \evecset{\rk+1}$, from the set of vectors which could legitimately be appended to $\tvvec$ (i.e. orthonormal to the existing eigenvectors with positive sign of the first element),
%
\begin{IEEEeqnarray}{rCl}
 \evecset{\rk+1} &=& \{\evec{} : |\evec{}|=1,\:\tvvec\tr\evec{}=0,\:\sign(\evec{\rk+1,1}=+1 \}    .
\end{IEEEeqnarray}
%
A new covariance matrix is then generated using the transformation,
%
\begin{IEEEeqnarray}{rCl}
 \lgtv\mcnew &=& \lgtv + \eval{\rk+1} \evec{\rk+1}\evec{\rk+1}\tr \\
 &=& \begin{bmatrix} \tvvec & \evec{\rk+1} \end{bmatrix} \begin{bmatrix} \tvval & \zmat \\ \zmat & \eval{\rk+1} \end{bmatrix} \begin{bmatrix} \tvvec\tr \\ \evec{\rk+1}\tr \end{bmatrix}
\end{IEEEeqnarray}
%
The inverse transformation, from $\lgtv\mcnew$ to $\{\lgtv, \eval{\rk+1}, \evec{\rk+1}\}$ simply requires identifying and removing the smallest eigenvalue and its corresponding eigenvector. This is an effective choice because the component with the smallest eigenvalue has the least effect on the likelihood of the observed data, and will thus have the highest probability of being accepted. 

For the sake of simplicity we suggest sampling the new eigenvalue and eigenvector independently from uniform distributions. For the eigenvalue,
%
\begin{IEEEeqnarray}{rCl}
 \ppslden{\eval{}}(\eval{\rk+1}|\lgtv) = \begin{cases}
                                          \frac{1}{\eval{\rk}} & 0 < \eval{\rk+1} < \eval{\rk} \\
                                          0 & \text{ otherwise}    .
                                         \end{cases}
\end{IEEEeqnarray}
%
For the eigenvector,
%
\begin{IEEEeqnarray}{rCl}
 \ppslden{\evec{}}(\evec{\rk+1}|\lgtv) = \begin{cases}
                                          \frac{1}{C} & \evec{\rk+1} \in \evecset{\rk+1} \\
                                          0 & \text{ otherwise}    .
                                         \end{cases}
\end{IEEEeqnarray}
%
$C$ is the volume of the manifold, which is half of a $(d-r-1)$-sphere (half because of the requirement that the sign of the first element be positive) \cite{Muirhead1982}. Hence,
%
\begin{IEEEeqnarray}{rCl}
 C &=& \frac{\pi^{\half(\lsd-\rk)}}{\gammafun\left(\frac{\lsd-\rk}{2}\right)}
\end{IEEEeqnarray}

The acceptance probability for this move and its reverse respectively are \cite{Green1995},
%
\begin{IEEEeqnarray}{rCl}
 \mhap(\lgtv,\eval{\rk+1},\evec{\rk+1}\to\lgtv\mcnew) &=& \min\left\{1, \beta \right\} \\
 \mhap(\lgtv\mcnew\to\lgtv,\eval{\rk+1},\evec{\rk+1}) &=& \min\left\{1, \beta\inv \right\} 
\end{IEEEeqnarray}
%
\begin{IEEEeqnarray}{rCl}
 \beta &=& \frac{ \postden(\lgtm, \lgtv\mcnew) \jac(\lgtv,\eval{\rk+1},\evec{\rk+1}\to\lgtv\mcnew) }{ \postden(\lgtm,\lgtv) \ppslden{\eval{}}(\eval{\rk+1}|\lgtv) \ppslden{\evec{}}(\evec{\rk+1}|\lgtv) }     ,
\end{IEEEeqnarray}
%
where $\jac(\lgtv,\eval{\rk+1},\evec{\rk+1}\to\lgtv\mcnew)$ is the Jacobian of the transformation.

The Jacobian of the eigendecomposition for a positive semidefinite matrix is \cite{Diaz-Garcia2006},
%
\begin{IEEEeqnarray}{rCl}
 \jac(\tvval,\tvvec\to\lgtv) &=& 2^{-\rk} \times \prod_{i=1}^{\rk}\eval{i}^{\lsd-\rk} \times \prod_{j=1}^{r}\prod_{i=1}^{j-1}(\eval{i}-\eval{j})      .
\end{IEEEeqnarray}
%
Repeating this for $\lgtv\mcnew$ and taking the ratio of the two, we obtain,
%
\begin{IEEEeqnarray}{rCl}
 \jac(\lgtv,\eval{\rk+1},\evec{\rk+1}\to\lgtv\mcnew) &=& 2^{-1} \eval{\rk+1}^{\lsd-\rk-1} \prod_{i=1}^{\rk}\eval{i}^{-1} \prod_{i=1}^{\rk}(\eval{i}-\eval{\rk+1})     .
\end{IEEEeqnarray}

\meta{I don't have $\rk$ in the distributions here. Sort that out.}











\section{Simulations}

\subsection{A Toy Model}

\subsection{Motion Capture Interpolation}

%%% REWRITING %%%


\section{Old Material}

The only tuning required is to select the Metropolis-Hastings proposal distributions for $\mcmckern{\lgtp,1}$ and $\mcmckern{\lgtm,\ls{}}$. In both cases these can be reduced to a single scale parameter; $\Qpadding$ in \eqref{eq:padded_transition_precision} for $\mcmckern{\lgtm,\ls{}}$ and $\sigma_s$ in \eqref{eq:skewsymmetric_proposal} for $\mcmckern{\lgtp,1}$. An adaptive MCMC scheme, such as those discussed in \cite{Roberts2009}, can be used to modify these values as the chain runs for maximum efficiency.






\section{Applications} \label{sec:applications}

\subsection{Example} \label{sec:example_basic}

We demonstrate the efficacy of the Gibbs sampler with a simple simulation example on a 4-dimensional model. The following transition matrix is used,
%
\begin{IEEEeqnarray}{rCl}
 \lgtm & = & \begin{bmatrix}
              0.9 & 0.7 & 0.7  & 0    \\
              0   & 0.9 & -0.5 & 0.1  \\
              0   & 0   & 1.6  & -0.8 \\
              0   & 0   & 1    & 0
             \end{bmatrix} \nonumber      .
\end{IEEEeqnarray}
%
The transfer function associated with $\lgtm$ has two real and two complex poles, all on or close to the unit circle. The transition covariance matrix is,
%
\begin{IEEEeqnarray}{rCl}
 \lgtv & = & \begin{bmatrix}
              1.0  & 0.5  & 0.5  & 0.5 \\
              0.5  & 1.0  & 0.5  & 0.5 \\ 
              0.5  & 0.5  & 1.0  & 0.5 \\
              0.5  & 0.5  & 0.5  & 1.0
             \end{bmatrix} \nonumber      .
\end{IEEEeqnarray}
%
For the observation model, $\lgom = \idmat$ and $\lgov = \idmat{}$.

A data set with $\timax=100$ time instants simulated from the model is shown in figure~\ref{fig:data_fullrank}. Running the Gibbs sampler results in rapid convergence. The resulting posterior histograms are shown in figure~\ref{fig:basic_FQ_histograms}, using $5000$ samples following a burn in of $1000$ iterations.

\begin{figure}
 \centering
% \includegraphics[width=0.9\columnwidth]{../code/data_fullrank.pdf}
 \caption{An example simulated data set. States (solid line) and observations (dots) of the four state components.}
 \label{fig:data_fullrank}
\end{figure}

\begin{figure}
 \centering
% \subfloat[]{\includegraphics[width=0.7\columnwidth]{../code/basic_F.pdf} \label{subf:basic_FQ_histograms:F}} \\
% \subfloat[]{\includegraphics[width=0.7\columnwidth]{../code/basic_Q.pdf} \label{subf:basic_FQ_histograms:Q}}
 \caption{MCMC Posterior histograms for \protect\subref{subf:basic_FQ_histograms:F} $\lgtm$ and \protect\subref{subf:basic_FQ_histograms:Q} $\lgtv$. Basic linear transition model. Vertical lines indicate true values}
 \label{fig:basic_FQ_histograms}
\end{figure}

\subsection{Example}

The algorithm was tested using the transition matrix specified in section~\ref{sec:example_basic}, but with the transition covariance changed to,
%
\begin{IEEEeqnarray}{rCl}
 \lgdm\lgdm\tr & = & \begin{bmatrix}
              0.26 &  0.33 &  0.26 & -0.42 \\
              0.33 &  1.14 &  0.46 & -0.85 \\ 
              0.26 &  0.46 &  0.29 & -0.48 \\
             -0.42 & -0.85 & -0.48 &  0.81
             \end{bmatrix} \nonumber      ,
\end{IEEEeqnarray}
%
(showing $2$ decimal places) which has a rank of $2$.

The MCMC algorithm was run using an adaptation scheme from \cite{Roberts2009}, adjusting the two proposal parameters based on the acceptance rate of the associated Metropolis-Hastings moves. Posterior histograms for $\lgtm$, $\lgtv$, and for the rank of $\lgtv$, are shown in figure~\ref{fig:degenerate_FQ_histograms} and~\ref{fig:degenerate_rank_histograms} for one MCMC run.

\begin{figure}
 \centering
 \subfloat[]{\includegraphics[width=0.7\columnwidth]{../code/degenerate_F_with_rank.pdf} \label{subf:degenerate_FQ_histograms:F}} \\
 \subfloat[]{\includegraphics[width=0.7\columnwidth]{../code/degenerate_Q_with_rank.pdf} \label{subf:degenerate_FQ_histograms:Q}}
 \caption{MCMC Posterior histograms for \protect\subref{subf:degenerate_FQ_histograms:F} $\lgtm$ and \protect\subref{subf:degenerate_FQ_histograms:Q} $\lgtv$. Degenerate transition model. Unknown rank.}
 \label{fig:degenerate_FQ_histograms}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=0.7\columnwidth]{../code/degenerate_rank_with_rank.pdf} 
 \caption{MCMC Posterior histogram for $\rk$. Degenerate transition model.}
 \label{fig:degenerate_rank_histograms}
\end{figure}



\subsection{Rigid-Body Modelling with Motion Capture} \label{sec:mocap}

In this section, the MCMC algorithm for degenerate transition models is applied to a simplified motion capture task. The data used is from \cite{Aristidou2013}. A motion capture system was used to measure the $3$-dimensional coordinates of a number of markers attached to a subject's body. Here we consider the four markers attached to the head. Since these are all attached to the same rigid body, we expect there to be substantial correlation in their motion. The linear model used is slightly modified from that introduced before. The latent state $\ls{\ti}$ consists of two parts,
%
\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \begin{bmatrix}
                 r_{\ti} \\ v_{\ti}
                \end{bmatrix} \nonumber 
\end{IEEEeqnarray}
%
where the vector $r_{\ti}$ denotes the positions of the markers and $v_{\ti}$ their corresponding velocities. A constant velocity model \cite{Li2003} is used with the following transition equation,
%
\begin{IEEEeqnarray}{rCl}
 \begin{bmatrix} r_{\ti} \\ v_{\ti} \end{bmatrix} & = & \begin{bmatrix} \idmat & \idmat \\ \zmat & \idmat \end{bmatrix} \begin{bmatrix} r_{\ti-1} \\ v_{\ti-1} \end{bmatrix} + \begin{bmatrix} \zmat \\ \lgdm \end{bmatrix} \tn{\ti} \nonumber \\
 \tn{\ti} & \sim & \normalden{\tn{\ti}}{0}{\idmat}      .
\end{IEEEeqnarray}
%
Our objective is to learn $\lgdm$. Defining $\lgtp$ as before, the MCMC kernels derived in section~\ref{sec:degenerate_transition_models} can all be applied. The measured marker positions are modelled as independent noisy observations of $r_t$, i.e. with,
%
\begin{IEEEeqnarray}{rCl}
 \lgov & = & \frac{1}{\xi} \idmat \nonumber     .
\end{IEEEeqnarray}
%
It is straightforward to Gibbs sample $\xi$; the posterior conditional is a Gamma distribution.

The original data is down-sampled to $5$Hz, and consists of $250$ time instants with occasional measurements missing due to occlusion. The full data set is shown in figure~\ref{fig:mocap_data}.

\begin{figure}
 \centering
% \includegraphics[width=0.9\columnwidth]{../code/mocap_data.pdf}
 \caption{Motion capture data. Showing the three position coordinates over time for each of the four head markers.}
 \label{fig:mocap_data}
\end{figure}

In order to test the learning algorithm, we discard the observations from one marker at a time in $4$ sections each of $20$ time instants (i.e. $4$s each). The remaining observations are used to learn the system parameters with MCMC. Kalman smoothing with the resulting model is then used to estimate the marker positions in the sections of missing data, and a root-mean-square-error may be calculated by comparing the estimated marker positions to the true values. 

The MCMC learning is run with $1000$ iterations, of which $500$ are discarded as burn-in. The sampler rapidly settles on a rank of $6$ and remains there for the rest of the chain. This accords with the fact that specifying the position of a solid body requires $6$ degrees of freedom. Figure~\ref{fig:mocap_Qhist} shows posterior histograms for a $6\times6$ block of the full covariance matrix.

\begin{figure}
 \centering
% \includegraphics[width=0.9\columnwidth]{../mocap/mocap_Qhist.pdf}
 \caption{Motion capture MCMC model learning. Posterior histograms for the elements of $\lgtv$ corresponding to the first two markers.}
 \label{fig:mocap_Qhist}
\end{figure}

Two other baseline methods are used for comparison: a Kalman smoother with $\lgdm=\idmat$, which corresponds to independent interpolation of each missing variable; and the missing value singular value decomposition algorithm \cite{Srebro2003,Liu2006,Li2009}, in which we use components sufficient to capture $95\%$ of the energy in each iteration. Results are shown in table~\ref{tab:mocap_rmse}. Figure~\ref{fig:missing_marker} shows one of the missing data sections and the estimates generated by the various methods.

\begin{table}[t]
\centering
\begin{tabular}{l|c}
 Algorithm                              & RMSE \\
 \hline
 Independent interpolation              & $0.505$ \\
 MSVD                                   & $0.101$ \\
 MCMC model mearning + Kalman Smoother  & $0.019$
\end{tabular}
\caption{RMSE results for missing marker estimation.}
\label{tab:mocap_rmse}
\end{table}

\begin{figure}
 \centering
% \includegraphics[width=0.9\columnwidth]{../code/missing_marker.pdf}
 \caption{First position coordinate of marker $1$, with data missing from time instant $90$ to $110$, showing ground truth (solid), Kalman smoother estimate with the learned model (mean as dashed, $\pm$2 standard deviations as dotted), independent interpolation (dash-dotted, upper), and MSVD (dash-dotted, lower).}
 \label{fig:missing_marker}
\end{figure}



\section{Conclusions}

Markov chain Monte Carlo algorithms are an effective method for Bayesian learning of linear state space systems. In this paper, we have introduced a number of new MCMC kernels for the linear system toolkit. New parameterisations have enabled us to formulate algorithms for learning transition models when it is known a priori that the model is (i) sparse or (ii) degenerate. The new algorithms are efficient and require minimal tuning, and have been verified with simulations.



\section*{Acknowledgements}
The authors are supported by the EPSRC BTaRoT grant. Our thanks to Michael Burke for advice and assistance with the motion capture application, and to Rich Wareham for his geometric insights.



\appendices

\section{A~Matrix~Factorisation for Positive~Semidefinite~Matrices} \label{app:givens-factorisation}

For a $\lsd\times\lsd$, rank-$\rk$ positive semi-definite precision matrix $\lgtp$, we write the full eigendecomposition as,
%
\begin{IEEEeqnarray}{rCl}
 \lgtp & = & \tnmofull \begin{bmatrix}
                        \tnmd & \zmat \\
                        \zmat & \zmat
                       \end{bmatrix} \tnmofull\tr \nonumber      .
\end{IEEEeqnarray}
%
where $\tnmofull$ is a square orthogonal matrix and $\tnmd$ is a diagonal matrix of $\rk$ eigenvalues.% This is not a unique factorisation, since $\lsd-\rk$ of the columns of $\tnmofull$ do not contribute to $\lgtp$ at all.

Next we consider factorising $\tnmofull$ using Givens rotations. A Givens rotation matrix has the following structure,
%
\begin{IEEEeqnarray}{rCl}
 \left[\givmat{i}{j}{\givrot{}} - \idmat\right]_{k,l} & = & \begin{cases}
                                                    \cos(\givrot{})-1 & k=l=i \text{ or } k=l=j \\
                                                    \sin(\givrot{}) & k=i,l=j \\
                                                    -\sin(\givrot{}) & k=j,l=i \\
                                                    0 & \text{ otherwise}     ,
                                                 \end{cases}
\end{IEEEeqnarray}
%
where $\givrot{} \in [-\pi/2,\pi/2]$ is a rotation in the plane of the $i$ and $j$ coordinate directions. Any orthogonal matrix may be written as a product of $\half d(d-1)$ such rotations in the following way \cite{Anderson1987},
%
\begin{IEEEeqnarray}{rCl}
\tnmosign &\times& \left[ \givmat{1}{2}{\givrot{1,2}}\right] \times \dotsm \nonumber\\
&\times& \left[\givmat{1}{d-1}{\givrot{1,d-1}} \dotsm \givmat{d-2}{d-1}{\givrot{d-2,d-1}} \right] \nonumber\\
&\times& \left[\givmat{1}{d}{\givrot{1,d}} \dotsm \givmat{d-1}{d}{\givrot{d-1,d}} \right] \label{eq:standard_givens}     ,
\end{IEEEeqnarray}
%
where $\tnmosign$ is a diagonal matrix in which the diagonal elements are $\pm1$. This factorisation may be derived (and also calculated) by iteratively multiplying the original matrix by a Givens rotation matrix such that one element is set to $0$. The matrix remaining once all the elements below the diagonal have been eliminated is $\tnmosign$, and the factorisation above results from a straightforward rearrangement. See \cite{Anderson1987} for details.

The order of the rotation matrices in the Givens factorisation is not unique. By successively eliminating elements using both left and right multiplication with Givens matrices, it is possible in the same way to show that there is a unique factorisation,
%
\begin{IEEEeqnarray}{rCl}
 \tnmofull & = & \tnmocross \tnmosign \tnmorow \tnmonull      ,
\end{IEEEeqnarray}
%
where
%
\begin{IEEEeqnarray}{rCl}
 \tnmonull & = & \left[\givmat{r+1}{r+2}{\givrot{r+1,r+2}}\right] \times \dotsm \nonumber \\
 & & \qquad\qquad \times \left[\givmat{r+1}{d}{\givrot{r+1,d}} \dots \givmat{d-1}{d}{\givrot{d-1,d}}\right] \nonumber \\
 \tnmorow  & = & \left[\givmat{1}{2}{\givrot{1,2}}\right] \times \dotsm \nonumber \\
 & & \qquad\qquad \times \left[\givmat{1}{r}{\givrot{1,r}} \dots \givmat{r-1}{r}{\givrot{r-1,r}}\right] \nonumber \\
 \tnmocross & = & \left[\givmat{1}{r+1}{\givrot{1,r+1}}\dots\givmat{1}{d}{\givrot{1,d}}\right] \times \dotsm \nonumber \\
 & & \qquad\qquad \times \left[\givmat{r}{r+1}{\givrot{r,r+1}} \dots \givmat{r}{d}{\givrot{r,d}}\right] \nonumber      ,
\end{IEEEeqnarray}
%
and $\tnmosign$ is as before. The terms $\tnmonull$ and $\tnmorow$ have the following particular form,
%
\begin{IEEEeqnarray}{rCl}
 \tnmonull & = & \begin{bmatrix}
                  \idmat[r\times r] & \zmat[r\times(d-r)] \\
                  \zmat[(d-r)\times r] & \tnmonullred
                 \end{bmatrix} \nonumber \\
 \tnmorow & = & \begin{bmatrix}
                  \tnmorowred & \zmat[r\times(d-r)] \\
                  \zmat[(d-r)\times r] & \idmat[(d-r)\times(d-r)]
                 \end{bmatrix} \nonumber      .
\end{IEEEeqnarray}
%
Substituting into the eigendecomposition,
%
\begin{IEEEeqnarray}{rCl}
 \lgtp & = & \tnmofull \begin{bmatrix}
                        \tnmd & \zmat \\
                        \zmat & \zmat
                       \end{bmatrix} \tnmofull\tr \nonumber \\
 & = & \tnmocross \tnmosign \tnmorow \tnmonull \begin{bmatrix}
                        \tnmd & \zmat \\
                        \zmat & \zmat
                       \end{bmatrix} \tnmonull\tr \tnmorow\tr \tnmosign\tr \tnmocross\tr \nonumber \\
 & = & \tnmocross \begin{bmatrix}
                        \tnmosignred\tnmorowred\tnmd\tnmorowred\tr\tnmosignred\tr & \zmat \\
                        \zmat & \zmat
                       \end{bmatrix} \tnmocross\tr \nonumber      ,
\end{IEEEeqnarray}
%
where $\tnmosignred$ is the top-left $r\times r$ block of $\tnmosign$. Comparing with \eqref{eq:standard_givens}, $\tnmosignred\tnmorowred$ can represent any $r\times r$ orthogonal matrix and hence $\tnmosignred\tnmorowred\tnmd\tnmorowred\tr\tnmosignred\tr$ is a unique positive definite matrix, which we write $\tnmf$. Finally, define $\tnmocrossred$ as the first $r$ columns of $\tnmocross$, and we reach,
%
\begin{IEEEeqnarray}{rCl}
 \lgtp & = & \tnmocrossred \tnmf \tnmocrossred\tr      .
\end{IEEEeqnarray}
%
Note that the space of possible values for $\tnmocrossred$ is restricted by the Givens factorisation. In particular, note that it is defined by $r(d-r)$ independent parameters. However, since we keep $\tnmocrossred$ fixed when using this factorisation, this restriction does not matter.



\bibliographystyle{IEEEtran}
\bibliography{/home/pete/Dropbox/PhD/bibliographies/Cleanbib}
% \bibliography{/users/pete/Dropbox/PhD/Cleanbib}
% \bibliography{/users/pete/Dropbox/PhD/OTbib}
% \bibliography{/home/pete/Dropbox/PhD/OTbib}
\end{document}



\end{document}
