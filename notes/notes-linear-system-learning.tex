\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{IEEEtrantools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{cite}
\usepackage[margin=1in]{geometry}
\usepackage{color}

\input{linear-system-learning-macros.tex}
\newcommand{\meta}[1]{{\color{red}\em #1}}

%opening
\title{Notes on Learning Linear State Space Models}
\author{Pete Bunch}

\begin{document}

\section{Basic Linear State Space}

\subsection{Definition}

\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \lgtm \ls{\ti-1} + \tn{\ti} \\
 \ob{\ti} & = & \lgom \ls{\ti}   + \on{\ti}
\end{IEEEeqnarray}
%
$\ls{\ti} \in \reals^{\lsd}$ and $\ob{\ti} \in \reals^{\obd}$.
%
\begin{IEEEeqnarray}{rCl}
 \tn{\ti} & \sim & \normaldist{0}{\lgtv} \\
 \on{\ti} & \sim & \normaldist{0}{\lgov}
\end{IEEEeqnarray}
%
$\lgtv$ and $\lgov$ are positive definite covariance matrices.

Equivalently in terms of transition and observation densities:
%
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) & = & \normalden{\ls{\ti}}{\lgtm\ls{\ti-1}}{\lgtv} \\
 \den(\ob{\ti}|\ls{\ti},\lgom,\lgov)   & = & \normalden{\ob{\ti}}{\lgom\ls{\ti}}{\lgov}
\end{IEEEeqnarray}
%
The initial state $\ls{1}$ may be known or may be assigned a Gaussian prior.



\subsection{Identifiability}



\subsection{Bayesian Learning with Gibbs Sampling}

Target the joint distribution $\den(\lgtm, \lgtv, \ls{1:\timax} | \ob{1:\timax}$. Gibbs sampling can be used, targeting the state and parameter posterior conditionals alternately. State conditional sampled by Kalman filtering and backward simulation.

\subsubsection{Parameter Conditional}

The conjugate prior is:
\begin{equation}
 \den(\lgtm, \lgtv) =  \den(\lgtm | \lgtv) \den(\lgtv)
\end{equation}
\begin{align}
 \lgtv &\sim \iwishartdist{\priordof}{\priorscalematrix} \\
 \lgtm | \lgtv &\sim \matrixnormaldist{\priormeanmatrix}{\lgtv}{\priorcolumnvariance}
\end{align}
%
So the prior densities are:
\begin{align}
\den(\lgtv) &= \frac{ \determ{\priorscalematrix}^{\frac{\priordof}{2}} }{ 2^{\frac{\priordof}{2}} \gammafun\left(\frac{\priordof}{2}\right) } \determ{\lgtv}^{-\frac{\priordof+\lsd+1}{2}} \exp\left( -\half \trace\left[\lgtv\inv\priorscalematrix\right] \right) \\
\den(\lgtm | \lgtv) &= \determ{2 \pi \lgtv}^{-\half} \determ{2 \pi \priorcolumnvariance}^{-\half} \exp\left(-\half \trace\left[ \lgtv\inv (\lgtm-\priormeanmatrix) \priorcolumnvariance\inv (\lgtm-\priormeanmatrix)\tr \right] \right) 
\end{align}
%
$\priordof\in\reals, \priordof>\lsd$. $\priorscalematrix$ and $\priorcolumnvariance$ are $\lsd\times\lsd$ positive definite matrices. $\priormeanmatrix \in \reals^{\lsd\times\lsd}$.

The likelihood function is:
\begin{align}
 \den(\ls{1:\timax}|\lgtm, \lgtv) &= \den(\ls{0}) \prod_{t=2}^{\timax} \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) \nonumber \\
 &\propto \determ{\lgtv}^{-\half(\timax-1)} \exp\left( -\half \sum_{t=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})\tr \lgtv\inv (\ls{\ti}-\lgtm\ls{\ti-1}) \right) \nonumber \\
 &= \determ{\lgtv}^{-\half(\timax-1)} \exp\left(-\half\trace\left[ \lgtv\inv \sum_{t=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})(\ls{\ti}-\lgtm\ls{\ti-1})\tr \right]\right) \nonumber \\
 &= \determ{\lgtv}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \lgtv\inv \left( \lgtm \suffstats{1} \lgtm\tr - \lgtm \suffstats{2}\tr - \suffstats{2} \lgtm\tr + \suffstats{3} \right) \right]\right)
\end{align}
%
where the sufficient statistics are:
%
\begin{align}
 \suffstats{0} &= \timax - 1 \\
 \suffstats{1} &= \sum_{t=2}^{\timax} \ls{\ti-1}\ls{\ti-1}\tr \\
 \suffstats{2} &= \sum_{t=2}^{\timax} \ls{\ti}\ls{\ti-1}\tr \\
 \suffstats{3} &= \sum_{t=2}^{\timax} \ls{\ti}\ls{\ti}\tr \\
\end{align}

The parameter posterior conditional is:
\begin{equation}
 \den(\lgtm, \lgtv | \ls{1:\timax}, \ob{1:\timax}) =  \den(\lgtm | \lgtv, \ls{1:\timax}) \den(\lgtv | \ls{1:\timax})
\end{equation}
\begin{align}
 \lgtv | \ls{1:\timax} &\sim \iwishartdist{\postdof}{\postscalematrix} \\
 \lgtm | \lgtv, \ls{1:\timax} &\sim \matrixnormaldist{\postmeanmatrix}{\lgtv}{\postcolumnvariance}
\end{align}

Hyperparameter updates:
\begin{align}
 \postcolumnvariance\inv                 &= \priorcolumnvariance\inv + \suffstats{1} \\
 \postmeanmatrix \postcolumnvariance\inv &= \priormeanmatrix \priorcolumnvariance\inv + \suffstats{2}\\
 \postdof                                &= \priordof + \suffstats{0} \\
 \postscalematrix                        &= \priorscalematrix + \suffstats{3} + \priormeanmatrix \priorcolumnvariance\inv \priormeanmatrix\tr - \postmeanmatrix \postcolumnvariance\inv \postmeanmatrix\tr
\end{align}

Proof:
\begin{IEEEeqnarray}{rCl}
 \den(\lgtm, \lgtv | \ls{1:\timax}, \ob{1:\timax}) &\propto& \den(\ls{1:\timax}|\lgtm, \lgtv) \den(\lgtm|\lgtv) \den(\lgtv) \nonumber \\
 &=& \determ{\lgtv}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \lgtv\inv \left( \lgtm \suffstats{1} \lgtm\tr - \lgtm \suffstats{2}\tr - \suffstats{2} \lgtm\tr + \suffstats{3} \right) \right]\right) \nonumber \\
 & & \times \determ{\lgtv}^{-\frac{\priordof+\lsd+1}{2}} \exp\left( -\half \trace\left[\lgtv\inv\priorscalematrix\right] \right) \nonumber \\
 & & \times \determ{\lgtv}^{-\half} \exp\left(-\half \trace\left[ \lgtv\inv (\lgtm-\priormeanmatrix) \priorcolumnvariance\inv (\lgtm-\priormeanmatrix)\tr \right] \right) \nonumber \\ 
 &=& \determ{\lgtv}^{-\frac{(\priordof+\suffstats{0})+\lsd+1}{2}} \exp\left( -\half \trace\left[\lgtv\inv(\priorscalematrix+\suffstats{3})\right] \right) \nonumber \\
 & & \times \determ{\lgtv}^{-\half} \exp\Bigg(-\half \trace\bigg[ \lgtv\inv \bigg( \lgtm (\priorcolumnvariance\inv + \suffstats{1}) \lgtm\tr - \lgtm (\priorcolumnvariance\inv\priormeanmatrix\tr + \suffstats{2}\tr) \nonumber \\
 & & \qquad\qquad\qquad\qquad\qquad  -\: (\priormeanmatrix\priorcolumnvariance\inv + \suffstats{2}) \lgtm\tr + \priormeanmatrix\priorcolumnvariance\inv\priormeanmatrix\tr \bigg) \bigg] \Bigg)
\end{IEEEeqnarray}
%
Finish by completing the square and comparing terms.



\section{Degenerate Linear State Space Model}

\subsection{Definition and Complications}

\begin{IEEEeqnarray}{rCl}
 \ls{\ti} & = & \lgtm \ls{\ti-1} + \lgdm \tn{\ti}
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \tn{\ti} & \sim & \normaldist{0}{\idmat}
\end{IEEEeqnarray}

This is equivalent to the basic case, except that $\lgtv = \lgdm\lgdm\tr$ is only positive semi-definite. This is not in itself a problem. The probability distribution associated with each transition is still well-defined, although it does not have a density on $\reals^{\lsd}$. The state perturbations $(\ls{\ti}-\lgtm\ls{\ti-1})$ must all lie in a linear subspace.

There are two problems with extending the Gibbs sampling methodology to such a model. First, the state sequence and the transition covariance matrix are too strongly related. The sampled state sequence defines the subspace, so we cannot freely change $\lgtv$. Second, the conjugate prior is tricky. The matrix-normal-inverse-wishart can be generalised to the case where $\lgtv$ is not full rank (using singular inverse Wishart and singular matrix normal distributions). However, this is not helpful, because it constrains the transition matrix to lie in a linear subspace.

We will need the eigen-decomposition of $\lgtv$ frequently.

\begin{IEEEeqnarray}{rCl}
 \lgtv &=& \tvvec \tvval \tvvec\tr
\end{IEEEeqnarray}
This is the ``non-singular'' part of the factorisation, where $\tvval$ is a diagonal $\rk\times\rk$ matrix with $\rk = \rank(\lgtv)$, and $\tvvec$ from the appropriate Stiefel manifold. The full decomposition will also be useful at times.

\begin{IEEEeqnarray}{rCl}
 \lgtv &=& \begin{bmatrix}\tvvec & \tvvecorth \end{bmatrix} \begin{bmatrix}\tvval & \zmat \\ \zmat & \zmat \end{bmatrix} \begin{bmatrix}\tvvec\tr \\ \tvvecorth\tr \end{bmatrix}
\end{IEEEeqnarray}


\subsection{Conjugate Prior}

\subsubsection{Inverse Wishart Part}

There are two ways to extend the inverse Wishart distribution to positive semi-definite matrices. Either let $\priordof \in \{z \in \naturals | z < \lsd\}$, in which case $\rank(\lgtv) = \priordof$, or let $\priorscalematrix$ be positive semi-definite. The latter makes it tricky to work out what we should set $\priorscalematrix$ to, so we'll go with the former. If we set $\priordof$ equal to a known value of the rank $\rk$, then the singular Wishart density is,

\begin{IEEEeqnarray}{rCl}
 \den(\lgtv|\rk) &=& \frac{ \determ{\priorscalematrix}^{\frac{\rk}{2}} }{ 2^{\half\rk\lsd} \pi^{\half\rk(\lsd-\rk)} \gammafun[r]\left(\frac{\rk}{2}\right) } \determ{\tvval}^{-\half(3\lsd-\rk+1)} \exp\left( -\half \lgtv\pinv \priorscalematrix \right)    .
\end{IEEEeqnarray}

\begin{itemize}
 \item \cite{Muirhead1982} covers the basic multivariate statistics for the full-rank case.
 \item \cite{Uhlig1994} covers singular Wishart distributions, including the eigenvalue distribution, and base measure for positive semi-definite matrices.
 \item \cite{Diaz-Garcia1997} covers singular and pseudo Wishart distributions in a thorough and unifying manner.
 \item \cite{Diaz-Garcia2006} covers singular and pseudo inverse Wishart distributions.
 \item \cite{Bodnar2008} contains a few useful properties.
 
\end{itemize}

Note that $\priorscalematrix$ is defined as the inverse of what it usually is, for consistency with the basic model case.



\subsubsection{Matrix Normal Part}

It's fine for the row-variance matrix in a matrix normal distribution to be less than full rank. That just means that the matrix is constrained to lie in a particular subspace. There's even a density associated with this. See \cite{Diaz-Garcia1997}. But this is no good as a prior. We want to be able to change the transition matrix freely.

Instead we can use,

\begin{IEEEeqnarray}{rCl}
 \lgtm|\lgtv &\sim& \matrixnormaldist{\priormeanmatrix}{\lgtv+\tvvecorth\priorextratpval\tvvecorth\tr}{\priorcolumnvariance}
\end{IEEEeqnarray}
%
$\priorextratpval$ is a diagonal matrix of positive eigenvalues which relax the constraint to lie in the subspace. They control the rate at which the density decays as we move away from that subspace. Setting these extra eigenvalues might be tricky. I haven't really worked out how to do this well yet. But they can depend on the other ones, so we could take the minimum, maximum or average of the others. Making them really big is uninformative.


\subsection{MCMC for Bayesian Learning}

A sampled state trajectory $\ls{1:\timax}$ defines the subspace in which the state perturbations must lie, which means that neither $\lgtm$ nor $\lgtv$ can be freely changed, and Gibbs sampling alone does not work. However, we can still sample within the space specified by this constraint. Additional MCMC moves will then be needed which allow the subspace to be changed.

\subsubsection{Factorising the Precision}

Using Givens rotations, we can efficiently factorise the precision eigenvector matrix as follows.
%
\begin{IEEEeqnarray}{rCl}
 \begin{bmatrix} \tvvec & \tvvecorth \end{bmatrix} &=& \begin{bmatrix}\tvrot & \tvrotorth\end{bmatrix}\begin{bmatrix} \tvsign & \zmat \\ \zmat & \tvsignorth \end{bmatrix} \begin{bmatrix} \tvrow & \zmat \\ \zmat & \idmat \end{bmatrix}\begin{bmatrix} \idmat & \zmat \\ \zmat & \tvnull \end{bmatrix}
\end{IEEEeqnarray}

So
%
\begin{IEEEeqnarray}{rCl}
 \tvvec     &=& \tvrot     \tvsign     \tvrow  \\
 \tvvecorth &=& \tvrotorth \tvsignorth \tvnull
\end{IEEEeqnarray}

Then defined the following, which is positive definite
%
\begin{equation}
 \tvfull = \tvsign \tvrow \tvval \tvrow\tr \tvsign\tr
\end{equation}

So
%
\begin{IEEEeqnarray}{rCl}
 \lgtv &=& \tvrot \tvfull \tvrot\tr
\end{IEEEeqnarray}



\subsubsection{State Trajectory Likelihood}

There is no proper transition density over $\reals^{\lsd}$ for the degenerate model. However, there is a density associated with the underlying state disturbances which allows us to write a likelihood function.

\begin{IEEEeqnarray}{rCl}
 \ls{\ti} &=& \lgtm \ls{\ti-1} + \tvrot \tvfull\msqrt \tn{\ti} \\
 \end{IEEEeqnarray}
 \begin{IEEEeqnarray}{rCl}
 \Rightarrow \tvrot\tr(\ls{\ti}-\lgtm\ls{\ti-1}) &\sim& \normaldist{\zmat}{\tvfull}
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) &\propto& \determ{\tvfull}^{-\half} \exp\left(-\half (\ls{\ti}-\lgtm\ls{\ti-1})\tr \tvrot \tvfull\inv \tvrot\tr (\ls{\ti}-\lgtm\ls{\ti-1}) \right)
\end{IEEEeqnarray}


The likelihood function associated with the state trajectory is:
\begin{align}
 \den(\ls{1:\timax}|\lgtm, \lgtv) &= \den(\ls{0}) \prod_{t=2}^{\timax} \den(\ls{\ti}|\ls{\ti-1},\lgtm,\lgtv) \nonumber \\
 &\propto \determ{\tvfull}^{-\half(\timax-1)} \exp\left( -\half \sum_{t=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})\tr \tvrot \tvfull\inv \tvrot\tr (\ls{\ti}-\lgtm\ls{\ti-1}) \right) \nonumber \\
 &= \determ{\tvfull}^{-\half(\timax-1)} \exp\left(-\half\trace\left[ \tvfull\inv \tvrot\tr \sum_{t=2}^{\timax} (\ls{\ti}-\lgtm\ls{\ti-1})(\ls{\ti}-\lgtm\ls{\ti-1})\tr \tvrot \right]\right) \nonumber \\
 &= \determ{\tvfull}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \tvfull\inv \left( \tvrot\tr \lgtm \suffstats{1} \lgtm\tr \tvrot - \tvrot\tr \lgtm \suffstats{2}\tr \tvrot - \tvrot\tr \suffstats{2} \lgtm\tr \tvrot + \tvrot\tr \suffstats{3} \tvrot \right) \right]\right) \\
 &= \determ{\tvfull}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \tvfull\inv \left( \lgtmrot \suffstats{1} \lgtmrot\tr - \lgtmrot \suffstats{2}\tr \tvrot - \tvrot\tr \suffstats{2} \lgtmrot\tr + \tvrot\tr \suffstats{3} \tvrot \right) \right]\right)
\end{align}
%
where
%
\begin{equation}
 \lgtmrot = \tvrot\tr \lgtm
\end{equation}

This is all subject to the constraint that each state must be reachable from the previous one, which we can write concisely as,
%
\begin{IEEEeqnarray}{rCl}
 \LS{2:\timax} &=& \lgtm \LS{1:\timax-1} + \tvrot Z
\end{IEEEeqnarray}
%
where $Z \in \reals^{\rk\times\lsd}$ and 
%
\begin{IEEEeqnarray}{rCl}
 \LS{2:\timax} &=& \begin{bmatrix} \ls{2} & \ls{3} & \hdots & \ls{\timax} \end{bmatrix}
\end{IEEEeqnarray}



\subsubsection{Transforming the Prior}

If we transform from $(\lgtm,\lgtv)$ to $(\lgtmrot,\tvfull)$, then the prior on these variables is as follows.

Using the transformation property of the matrix normal distribution:
%
\begin{IEEEeqnarray}{rCl}
 \lgtmrot|\lgtv = \lgtmrot|\tvrot,\tvfull &\sim& \matrixnormaldist{\tvrot\tr\priormeanmatrix}{\tvfull}{\priorcolumnvariance}
\end{IEEEeqnarray}

Using the transformation property of the Wishart distribution:
%
\begin{IEEEeqnarray}{rCl}
 \tvfull|\tvrot &\sim& \iwishartdist{\rk}{(\tvrot\tr\priorscalematrix\inv\tvrot)\inv}
\end{IEEEeqnarray}

Proof:
%
\begin{IEEEeqnarray}{rCl}
 X &\sim& \wishartdist{\nu}{\Sigma} \nonumber\\
 \Rightarrow Z &\sim& \matrixnormaldist{\zmat}{\idmat}{\idmat} \nonumber\\
 \text{with } X &=& \Sigma\msqrt Z Z\tr {\Sigma\msqrt}\tr \nonumber \\
 \text{and } Z &\in& \reals^{d\times \nu} \nonumber\\
 Y = X\pinv &=& {\Sigma\msqrt}\inv (Z Z\tr)\pinv {\Sigma\msqrt}\invtr \nonumber \\
 Y &\sim& \iwishartdist{\nu}{\Sigma} \nonumber \\
 A Y A\tr &=& A {\Sigma\msqrt}\inv (Z Z\tr)\pinv {\Sigma\msqrt}\invtr A\tr \nonumber \\
 &=& (A{\Sigma\msqrt}\inv) (Z Z\tr)\pinv (A{\Sigma\msqrt}\inv)\tr \nonumber \\
 \Rightarrow A Y A\tr &\sim& \iwishartdist{\nu}{ \left((A{\Sigma\msqrt}\inv)(A{\Sigma\msqrt}\inv)\tr\right)\inv } \nonumber \\
 A Y A\tr &\sim& \iwishartdist{\nu}{ \left((A\Sigma\inv A\tr\right)\inv } \nonumber \\
\end{IEEEeqnarray}





\subsubsection{Within-Subspace Parameter Conditional}

We can now get to the conditional distribution for $\den(\lgtmrot, \tvfull|\tvrot,\ls{1:\timax})$ which is well-behaved and can be sampled.

The parameter posterior conditional is:
\begin{equation}
 \den(\lgtmrot, \tvfull | \tvrot, \ls{1:\timax}, \ob{1:\timax}) =  \den(\lgtmrot | \tvfull, \tvrot, \ls{1:\timax}) \den(\tvfull | \tvrot, \ls{1:\timax})
\end{equation}
\begin{align}
 \tvfull | \tvrot, \ls{1:\timax} &\sim \iwishartdist{\postdof}{\postscalematrix} \\
 \lgtmrot | \tvfull, \tvrot, \ls{1:\timax} &\sim \matrixnormaldist{\postmeanmatrix}{\lgtv}{\postcolumnvariance}
\end{align}

Hyperparameter updates:
\begin{align}
 \postcolumnvariance\inv                 &= \priorcolumnvariance\inv + \suffstats{1} \\
 \postmeanmatrix \postcolumnvariance\inv &= \tvrot\tr\left( \priormeanmatrix \priorcolumnvariance\inv + \suffstats{2} \right) \\
 \postdof                                &= \rk + \suffstats{0} \\
 \postscalematrix                        &= (\tvrot\tr\priorscalematrix\inv\tvrot)\inv + \tvrot\tr\left( \suffstats{3} + \priormeanmatrix \priorcolumnvariance\inv \priormeanmatrix\tr \right)\tvrot - \postmeanmatrix \postcolumnvariance\inv \postmeanmatrix\tr
\end{align}

Proof:
\begin{IEEEeqnarray}{rCl}
 \den(\lgtmrot, \tvfull|\tvrot,\ls{1:\timax}) &\propto& \den(\ls{1:\timax}|\lgtmrot, \tvfull, \tvrot) \den(\lgtmrot | \tvfull, \tvrot) \den(\tvfull|\tvrot) \nonumber \\
%
 &\propto& \determ{\tvfull}^{-\half\suffstats{0}} \exp\left(-\half\trace\left[ \tvfull\inv \left( \lgtmrot \suffstats{1} \lgtmrot\tr - \lgtmrot \suffstats{2}\tr \tvrot - \tvrot\tr \suffstats{2} \lgtmrot\tr + \tvrot\tr \suffstats{3} \tvrot \right) \right]\right) \nonumber \\
 & & \times \determ{\tvfull}^{-\half(r+r+1)} \exp\left( -\half \trace\left[\tvfull\inv (\tvrot\tr\priorscalematrix\inv\tvrot)\inv \right] \right) \nonumber \\
 & & \times \determ{\tvfull}^{-\half} \exp\left(-\half \trace\left[ \tvfull\inv (\lgtmrot-\tvrot\tr\priormeanmatrix) \priorcolumnvariance\inv (\lgtmrot-\tvrot\tr\priormeanmatrix)\tr \right] \right) \nonumber \\ 
%
 &=& \determ{\tvfull}^{-\half\left((r+\suffstats{0})+r+1\right)} \exp\left( -\half \trace\left[\tvfull\inv \left( (\tvrot\tr\priorscalematrix\inv\tvrot)\inv + \tvrot\tr\suffstats{3}\tvrot \right)\right] \right) \nonumber \\
 & & \times \determ{\tvfull}^{-\half} \exp\Bigg(-\half \trace\bigg[ \tvfull\inv \bigg( \lgtmrot (\priorcolumnvariance\inv + \suffstats{1}) \lgtmrot\tr - \lgtmrot(\priorcolumnvariance\inv\priormeanmatrix\tr + \suffstats{2}\tr) \tvrot \nonumber \\
 & & \qquad\qquad\qquad\qquad\qquad  -\: \tvrot\tr (\priormeanmatrix\priorcolumnvariance\inv + \suffstats{2}) \lgtmrot\tr + \tvrot\tr \priormeanmatrix\priorcolumnvariance\inv\priormeanmatrix\tr \tvrot \bigg) \bigg] \Bigg)
\end{IEEEeqnarray}
%
Finish by completing the square and comparing terms.

Of course, we want $\lgtm$, not $\lgtmrot$. We can recover the matrix uniquely using the constraint from the likelihood. The previous value of $\lgtm$ from before we sampled must have satisfied this constraint. Let's call that matrix $\lgtm\mcold$. 
%
\begin{IEEEeqnarray}{rCl}
 \LS{2:\timax} &=& \lgtm\mcold \LS{1:\timax-1} + \tvrot Z\mcold \nonumber \\
 \LS{2:\timax} &=& \lgtm \LS{1:\timax-1} + \tvrot Z \nonumber \\
 \Rightarrow \zmat &=& (\lgtm-\lgtm\mcold) \LS{1:\timax-1} + \tvrot(Z-Z\mcold) \\
 \Rightarrow \zmat &=& (\lgtmrot-\lgtmrot\mcold) \LS{1:\timax-1} + (Z-Z\mcold) \\
 \Rightarrow \zmat &=& (\lgtm-\lgtm\mcold) \LS{1:\timax-1} - \tvrot (\lgtmrot-\lgtmrot\mcold) \LS{1:\timax-1} \\
 \zmat &=& \left[(\lgtm-\lgtm\mcold) - \tvrot (\lgtmrot-\lgtmrot\mcold)\right] \LS{1:\timax-1} \\
 \Rightarrow \zmat &=& (\lgtm-\lgtm\mcold) - \tvrot (\lgtmrot-\lgtmrot\mcold)
\end{IEEEeqnarray}
\begin{IEEEeqnarray}{rCl}
 \lgtm &=& \lgtm\mcold + \tvrot (\lgtmrot-\lgtmrot\mcold) \nonumber \\
       &=& \lgtm\mcold + \tvrot (\lgtmrot-\tvrot\tr\lgtm\mcold) \nonumber \\
       &=& (\idmat - \tvrot\tvrot\tr) \lgtm\mcold + \tvrot \lgtmrot 
\end{IEEEeqnarray}



\subsubsection{Metropolis-Hastings for the Covariance Subspace}

In order to change $\tvrot$, we can target $\den(\lgtv|\lgtm, \ob{1:\timax})$ with Metropolis-Hastings, which is justified as a collapsed Gibbs move.

We sample a rotation matrix $\mhrot$ from some proposal distribution $\mhrotppsl$ and then apply the following transformation,
%
\begin{IEEEeqnarray}{rCl}
 \begin{bmatrix}
  \lgtv\mcnew \\ \mhrot\mcnew
 \end{bmatrix}
 & = &
 \begin{bmatrix}
  \mhrot\lgtv\mhrot\tr \\ \mhrot\tr
 \end{bmatrix} \nonumber     ,
\end{IEEEeqnarray}
%
which is clearly its own inverse. Since $\determ{\mhrot}=1$, it is straightforward to show that the Jacobian of this transformation is $1$, and hence using the reversible jump interpretation of Metropolis-Hastings \cite{Green1995,Green2009}, the acceptance probability is,
%
\begin{IEEEeqnarray}{rCl}
 \mhap(\lgtv\to\lgtv\mcnew) & = & \min\left\{1, \frac{ \den(\lgtv\mcnew|\lgtm,\ob{1:\timax})\mhrotppsl(\mhrot\mcnew) }{ \den(\lgtv|\lgtm, \ob{1:\timax})\mhrotppsl(\mhrot) } \right\}  \\
 & = & \min\left\{1, \frac{\den(\ob{1:\timax}|\lgtm,\lgtv\mcnew)}{\den(\ob{1:\timax}|\lgtm,\lgtv)} \times \frac{\den(\lgtv\mcnew, \lgtm) }{\den(\lgtv, \lgtm)} \times \frac{\mhrotppsl(\mhrot\mcnew)}{\mhrotppsl(\mhrot)} \right\} \nonumber     .
\end{IEEEeqnarray}
%
The first term is simply a ratio of Kalman filter likelihoods.

There are numerous ways to sample the rotation matrix $\mhrot$ from a suitable proposal distribution $\mhrotppsl$. For example, we could use the Cayley transform \cite{Leon2006}, a bijective mapping from the skew-symmetric matrices to the rotation matrices, defined by,
%
\begin{IEEEeqnarray}{rCl}
 P(S) & = & (\idmat - S)\inv(I+S)     .
\end{IEEEeqnarray}
%
To sample from $\mhrotppsl$, we draw $\half\lsd(\lsd-1)$ independent scalar random variables $\{s_{i,j}\}_{0<i<j<\lsd}$ from any zero-mean distribution; a nice choice is,
%
\begin{IEEEeqnarray}{rCl}
 s_{k,l} & \sim & \normaldist{0}{\sigma_s^2} \label{eq:skewsymmetric_proposal}     .
\end{IEEEeqnarray}
%
Use these to construct a skew-symmetric matrix $S$,
%
\begin{IEEEeqnarray}{rCl}
 S_{k,l} & = & \begin{cases}
                s_{k,l}  & k<l \\
                -s_{l,k} & k>l \\
                0        & k=l     ,
               \end{cases}
\end{IEEEeqnarray}
%
and then set $\mhrot=P(S)$. The Cayley transform has the property that $P(-S)=P(S\tr)=P(S)\inv=P(S)\tr$, which implies that $\mhrotppsl(\mhrot)=\mhrotppsl(\mhrot\tr)=\mhrotppsl(\mhrot\mcnew)$, leading to a cancellation in the acceptance probability.

There is an alternative using Givens rotations. First sample $i \in [1,\lsd]$, $j \in [1,\lsd]\setminus i$, and $\givrot{} \in [-\pi/2,\pi/2]$ from some zero-mean distribution. Form the Givens matrix $\givmat{i}{j}{\givrot{}}$ such that,
%
\begin{IEEEeqnarray}{rCl}
 \left[\givmat{i}{j}{\givrot{}} - \idmat\right]_{k,l} & = & \begin{cases}
                                                    \cos(\givrot{})-1 & k=l=i \text{ or } k=l=j \\
                                                    \sin(\givrot{}) & k=i,l=j \\
                                                    -\sin(\givrot{}) & k=j,l=i \\
                                                    0 & \text{ otherwise,}
                                                 \end{cases}
\end{IEEEeqnarray}
%
and use $\mhrot=\givmat{i}{j}{\givrot{}}$. This also has the property that $\givmat{i}{j}{-\givrot{}} = \givmat{i}{j}{\givrot{}}\tr$, meaning that we achieve the same cancellation of the proposals as before.



\subsubsection{Metropolis-Hastings for the Transition Matrix}

Define $\postden$ as the posterior density, conditioning on $\ob{1:\timax}$.

If we modify the covariance matrix slightly so that it is no longer singular, then we can use the existing sampling procedure from the basic linear model and treat the resulting value of $\lgtm$ as a proposal in a Metropolis-Hastings kernel targeting $\postden(\lgtm, \ls{1:\timax}|\lgtv)$.

Suppose we start with sampled values of $\lgtm$,$\lgtv$ and $\ls{1:\timax}$. A suitably modified non-singular covariance matrix is,
%
\begin{IEEEeqnarray}{rCl}
 \paddedlgtv & = & \lgtv + \padding \idmat \label{eq:padded_transition_covariance}      ,
\end{IEEEeqnarray}
%
where $\padding$ is a small positive constant. We propose a new value of $\lgtm$ by sampling,
%
\begin{IEEEeqnarray}{rCl}
 \lgtm' & \sim & \postden(\lgtm|\paddedlgtv,\ls{1:\timax})     .
\end{IEEEeqnarray}
%
This distribution is simply a Gaussian and we can evaluate the density using \eqref{eq:basic_F_conditional}. Finally, we also sample a new state trajectory using forward filtering backward sampling,
%
\begin{IEEEeqnarray}{rCl}
 \ls{1:\timax}' & \sim & \postden(\ls{1:\timax}|\lgtm', \lgtv)      .
\end{IEEEeqnarray}
%
The acceptance probability is simply,
%
\begin{IEEEeqnarray}{rCl}
 \IEEEeqnarraymulticol{3}{l}{ \mhap\left(\lgtm,\ls{1:\timax}\to\lgtm',\ls{1:\timax}'\right) } \nonumber \\
 \quad\quad & = & \min\left\{1,\frac{ \postden(\lgtm',\ls{1:\timax}'|\lgtv) \postden(\ls{1:\timax}|\lgtm,\lgtv) \postden(\lgtm|\paddedlgtv,\ls{1:\timax}')   }{ \postden(\lgtm,\ls{1:\timax}|\lgtv) \postden(\ls{1:\timax}'|\lgtm',\lgtv) \postden(\lgtm'|\paddedlgtv,\ls{1:\timax}) }\right\} \nonumber \\
 & = & \min\left\{1,\frac{ \postden(\lgtm'|\lgtv) \postden(\lgtm|\paddedlgtv,\ls{1:\timax}')   }{ \postden(\lgtm|\lgtv)\postden(\lgtm'|\paddedlgtv,\ls{1:\timax}) }\right\} \nonumber \\
 & = & \min\left\{1, \frac{ \den(\ob{1:\timax}|\lgtm',\lgtv) }{ \den(\ob{1:\timax}|\lgtm,\lgtv)} \times \frac{ \den(\lgtm'|\lgtv) }{ \den(\lgtm|\lgtv)} \times \frac{ \postden(\lgtm|\paddedlgtv,\ls{1:\timax}')   }{ \postden(\lgtm'|\paddedlgtv,\ls{1:\timax}) }\right\} \nonumber      , \\
\end{IEEEeqnarray}
%
in which the first term is a ratio of Kalman filter likelihoods.






\appendix


\bibliographystyle{plain}
\bibliography{/home/pete/Dropbox/PhD/bibliographies/OTbib}
\end{document}

